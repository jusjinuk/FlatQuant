diff --git a/.gitignore b/.gitignore
index 154d7b8..c4d9608 100644
--- a/.gitignore
+++ b/.gitignore
@@ -15,3 +15,4 @@ Makefile
 log_*.txt
 *.pth
 modelzoo/*
+outputs/*
diff --git a/benchmarks/benchmark_model.py b/benchmarks/benchmark_model.py
new file mode 100644
index 0000000..a3302b4
--- /dev/null
+++ b/benchmarks/benchmark_model.py
@@ -0,0 +1,515 @@
+import argparse
+import gc
+import pprint
+import numpy as np
+import torch
+import time
+import os
+
+import deploy.transformers.modeling_llama as modeling_llama
+import torch
+import transformers
+from safetensors.torch import load_file
+import json
+
+import flatquant.data_utils as data_utils
+from tqdm import tqdm
+
+model_configs = [
+    # "./modelzoo/llama-2-hf/llama-2-7b-hf",
+    "./modelzoo/llama-2-hf/llama-2-13b-hf",
+    #"./modelzoo/llama-3/llama-3-8b", 
+    #"./modelzoo/llama-3-instruct/llama-3-8b-instruct"
+]
+
+benchmark_dtypes = ["int4", torch.float16]
+num_warmup_steps = 2
+num_bench_steps = 1
+
+def repeated_run(num_repeats=10):
+    def func(module):
+        def _f(*args, **kwargs):
+            times = []
+            for i in range(num_repeats):
+                times.append(module(*args, **kwargs, repeat_idx = i))
+            return tuple(zip(*times))
+        return _f
+    return func
+
+def _cleanup():
+    gc.collect()
+    torch.cuda.empty_cache()
+
+@repeated_run()
+def module_benchmark(module, repeat_idx):
+    # warmup
+    for i in range(num_warmup_steps):
+        out = module()
+    torch.cuda.synchronize()
+    
+    _cleanup()
+    torch.cuda.reset_max_memory_allocated()
+    start_time = time.perf_counter()
+    
+    
+    for i in range(num_bench_steps):
+        out = module()
+    torch.cuda.synchronize()
+    peak_memory = torch.cuda.max_memory_allocated()
+
+    end_time = time.perf_counter()
+
+    return (end_time - start_time) * 1000 / num_bench_steps, peak_memory
+
+def load_dataset(config_name):
+    for eval_dataset in ["wikitext2"]:
+        testloader = data_utils.get_loaders(
+                    args = None,
+                    name = eval_dataset,
+                    model = config_name,
+                    seqlen = 2048,
+                    eval_mode = True
+                )
+    return testloader
+
+@torch.no_grad()
+def ppl_eval(model, testenc):
+    print('Evaluating ppl...')
+    model.eval()
+    model = model.cuda()
+    max_length = 2048   # fix model max length
+
+    testenc = testenc.input_ids
+    nsamples = testenc.numel() // max_length
+
+    dev = next(model.parameters()).device
+
+    testenc = testenc.to(dev)
+
+    # warmup
+    for i in range(num_warmup_steps):
+        batch = testenc[:, (i * max_length): ((i + 1) * max_length)]
+        out = model(batch)
+    torch.cuda.synchronize()
+    _cleanup()
+
+    nlls = []
+    inference_times = []
+    
+    for i in range(nsamples):
+        batch = testenc[:, (i * max_length): ((i + 1) * max_length)]
+
+        torch.cuda.synchronize()
+        start_time = time.perf_counter()
+
+        lm_logits = model(batch).logits
+
+        torch.cuda.synchronize()
+        end_time = time.perf_counter()
+
+        inference_times.append((end_time - start_time) * 1000)
+
+        shift_logits = lm_logits[:, :-1, :].contiguous()
+        shift_labels = testenc[
+            :, (i * max_length): ((i + 1) * max_length)
+        ][:, 1:].to(shift_logits.device)
+
+        loss_fct = torch.nn.CrossEntropyLoss()
+        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
+        neg_log_likelihood = loss.float() * max_length
+        nlls.append(neg_log_likelihood)
+    ppl = torch.exp(torch.stack(nlls).sum() / (len(nlls) * max_length))
+    avg_time = np.mean(inference_times)
+    std_time = np.std(inference_times)
+
+    return ppl.item(), avg_time, std_time
+
+# Load from safetensors format
+def load_from_safetensors(checkpoint_path):
+
+    from safetensors import safe_open
+    from safetensors.torch import load_file
+
+    state_dict = {}
+    
+    if os.path.isdir(checkpoint_path):
+        # Check for index file (sharded model)
+        index_path = os.path.join(checkpoint_path, "model.safetensors.index.json")
+        single_path = os.path.join(checkpoint_path, "model.safetensors")
+        
+        if os.path.exists(index_path):
+            # Sharded model - load index
+            with open(index_path, 'r') as f:
+                index = json.load(f)
+            
+            # Load all shards
+            loaded_files = set()
+            
+            for tensor_name, filename in index["weight_map"].items():
+                if filename not in loaded_files:
+                    shard_path = os.path.join(checkpoint_path, filename)
+                    with safe_open(shard_path, framework="pt") as f:
+                        for key in f.keys():
+                            if key in index["weight_map"] and index["weight_map"][key] == filename:
+                                state_dict[key] = f.get_tensor(key)
+                    loaded_files.add(filename)
+            
+        else:
+            # Single file
+            state_dict = load_file(single_path)
+    
+    # Reconstruct the checkpoint format from safetensors
+    checkpoint = {
+        "model_state_dict": {},
+        "quantizers": {}
+    }
+    
+    for k, v in state_dict.items():
+        if k.startswith("quantizer."):
+            parts = k.split(".")
+            layer_name = ".".join(parts[1:-1])
+            param_type = parts[-1]
+            
+            if param_type == "scale":
+                if layer_name not in checkpoint["quantizers"]:
+                    class Quantizer:
+                        pass
+                    checkpoint["quantizers"][layer_name] = Quantizer()
+                checkpoint["quantizers"][layer_name].scale = v
+        else:
+            checkpoint["model_state_dict"][k] = v
+    
+    return checkpoint
+
+# rename parameters for loading
+def rename_keys(checkpoint):
+    new_checkpoint = {
+        "model_state_dict": {},
+        "quantizers": {}
+    }
+    for k, v in checkpoint["model_state_dict"].items():
+        new_k = k.replace("q_proj.linear", "q_proj") \
+                 .replace("q_proj.act_quantizer", "inp_trans_q") \
+                 .replace("k_proj.linear", "k_proj") \
+                 .replace("k_proj.act_quantizer", "inp_trans_k") \
+                 .replace("v_proj.linear", "v_proj") \
+                 .replace("v_proj.act_quantizer", "inp_trans_v") \
+                 .replace("o_proj.linear", "o_proj.1") \
+                 .replace("o_proj.act_quantizer", "o_proj_trans") \
+                 .replace("ln_trans.matrix_left", "left_matrix") \
+                 .replace("ln_trans.matrix_right", "right_matrix") \
+                 .replace("ln_trans", "inp_trans_k") \
+                 .replace("o_trans.matrix", "o_proj_trans.right_matrix") \
+                 .replace("gate_proj.linear", "gate_proj") \
+                 .replace("gate_proj.act_quantizer", "inp_trans_g") \
+                 .replace("up_proj.linear", "up_proj") \
+                 .replace("up_proj.act_quantizer", "inp_trans_u") \
+                 .replace("down_proj.linear", "down_proj.2") \
+                 .replace("down_proj.act_quantizer", "down_proj.0") \
+                 .replace("down_trans.matrix_left", "down_proj.0.left_matrix") \
+                 .replace("down_trans.matrix_right", "down_proj.0.right_matrix")\
+                 .replace("down_trans", "down_proj.0") \
+                 .replace("up_gate_trans.matrix_left", "left_matrix") \
+                 .replace("up_gate_trans.matrix_right", "right_matrix") \
+                 .replace("up_gate_trans", "inp_trans_g") \
+                 .replace("k_cache_quantizer.clip", "kclip") \
+                 .replace("v_cache_quantizer.clip", "vclip") \
+                 .replace("kcache_trans.matrix", "trans_matrix_k") \
+                 .replace("vcache_trans.matrix", "trans_matrix_v")
+        new_checkpoint["model_state_dict"][new_k] = v
+    
+    for k, v in checkpoint["quantizers"].items():
+        new_k = k.replace("linear", "weight_scales") \
+                 .replace("mlp.down_proj.weight_scales", "mlp.down_proj.2.weight_scales") \
+                 .replace("self_attn.o_proj.weight_scales", "self_attn.o_proj.1.weight_scales")
+        new_checkpoint["quantizers"][new_k] = v.scale
+
+    return new_checkpoint
+
+def get_model_quantized(args, config_name, checkpoint_path = None):
+    config = transformers.AutoConfig.from_pretrained(
+        config_name,
+        attn_implementation="flash_attention_2"
+    )
+    dtype_old = torch.get_default_dtype()
+    torch.set_default_dtype(torch.float16)
+    with transformers.modeling_utils.no_init_weights():
+        model = modeling_llama.FlatQuantLlamaForCausalLM(args=args, config=config)
+    if checkpoint_path:
+        checkpoint = load_from_safetensors(checkpoint_path)
+        new_checkpoint = rename_keys(checkpoint = checkpoint)
+        missing_keys_1, unexpected_keys_1 = model.load_state_dict(new_checkpoint["model_state_dict"], strict=False)
+        missing_keys_2, unexpected_keys_2  = model.load_state_dict(new_checkpoint['quantizers'], strict = False)
+
+        
+        for layer in model.model.layers:    
+            layer.self_attn.inp_trans_q.register_buffer("left_matrix", layer.self_attn.left_matrix)
+            layer.self_attn.inp_trans_k.register_buffer("left_matrix", layer.self_attn.left_matrix)
+            layer.self_attn.inp_trans_v.register_buffer("left_matrix", layer.self_attn.left_matrix)
+            layer.self_attn.inp_trans_q.register_buffer("right_matrix", layer.self_attn.right_matrix)
+            layer.self_attn.inp_trans_k.register_buffer("right_matrix", layer.self_attn.right_matrix)
+            layer.self_attn.inp_trans_v.register_buffer("right_matrix", layer.self_attn.right_matrix)
+
+            layer.mlp.inp_trans_u.register_buffer("left_matrix", layer.mlp.left_matrix)
+            layer.mlp.inp_trans_u.register_buffer("right_matrix", layer.mlp.right_matrix)
+            layer.mlp.inp_trans_g.register_buffer("left_matrix", layer.mlp.left_matrix)
+            layer.mlp.inp_trans_g.register_buffer("right_matrix", layer.mlp.right_matrix)
+
+        for name, module in model.named_modules():
+            for attr_name in ['clip_factor_a_max', 'clip_factor_a_min']:
+                if hasattr(module, attr_name):
+                    attr_value = getattr(module, attr_name)
+                    if isinstance(attr_value, torch.Tensor):
+                        delattr(module, attr_name)
+                        setattr(module, attr_name, attr_value.item())
+
+        missing_keys_both = set(missing_keys_1) & set(missing_keys_2)
+        print("success to load real weights")
+
+    torch.set_default_dtype(dtype_old)
+    return model
+
+
+def get_model_hf(config_name):
+    return transformers.LlamaForCausalLM.from_pretrained(
+        config_name, 
+        torch_dtype=torch.float16, 
+        attn_implementation="flash_attention_2"
+    )
+
+def get_model_fp16(config_name):
+    return modeling_llama.FlatQuantFP16LlamaForCausalLM.from_pretrained(
+        config_name, 
+        torch_dtype=torch.float16, 
+        attn_implementation="flash_attention_2"
+    )
+
+
+def run_prefill(model, bsz, prefill_length):
+    device = model.device
+    test_input = torch.randint(100, 200, (bsz, prefill_length), dtype=torch.int32, device=device)
+    return module_benchmark(lambda: model(test_input))
+
+
+def run_decode(model, bsz, prefill_length, decode_steps):
+    device = model.device
+    test_input = torch.randint(100, 200, (bsz, prefill_length), dtype=torch.int32, device=device)
+    model._expected_max_length = prefill_length + decode_steps
+    out = model(test_input)
+    past_key_values = out.past_key_values
+    del out
+    _cleanup()
+    next_input = torch.tensor([[100] for _ in range (bsz)], dtype=torch.int32, device=device)
+    def _decode_for_multiple_steps():
+        past_key_values.length = prefill_length
+        for _ in range(decode_steps):
+            model(next_input, past_key_values=past_key_values)
+    return module_benchmark(_decode_for_multiple_steps)
+    
+
+def run_e2e(model, bsz, prefill_length, decode_steps):
+    device = model.device
+    test_input = torch.randint(100, 200, (bsz, prefill_length), dtype=torch.int32, device=device)
+    next_input = torch.tensor([[100] for _ in range (bsz)], dtype=torch.int32, device=device)
+    def _prefill_and_decode_for_multiple_steps():
+        model._expected_max_length = prefill_length + decode_steps
+        out = model(test_input)
+        for _ in range(decode_steps):
+            model(next_input, past_key_values=out.past_key_values)
+    return module_benchmark(_prefill_and_decode_for_multiple_steps)
+
+
+def _wait_for_input():
+    print("Press enter")
+    input()
+
+@torch.no_grad
+def run_all_for_model(model, bsz, prefill, decode):
+    model.eval()
+    model = model.cuda()
+    time_prefill, _ = run_prefill(model, bsz, prefill)
+    _cleanup()
+    if decode is not None:
+        time_decode, memory_decode = run_decode(model, bsz, prefill, decode)
+        _cleanup()
+        time_e2e, _ = run_e2e(model, bsz, prefill, decode)
+        _cleanup()
+    else:
+        time_decode = time_e2e = None
+    return time_prefill, time_decode, time_e2e, memory_decode
+
+
+def print_e2e_time(args, time_prefill_i4, time_decode_i4, time_e2e_i4, time_prefill_f16, time_decode_f16, time_e2e_f16,
+                   time_prefill_i4_benchmark=None, time_decode_i4_benchmark=None, time_e2e_i4_benchmark=None):
+    prefill_speedup = np.mean(time_prefill_f16) / np.mean(time_prefill_i4)
+    prefill_benchmark_speedup = np.mean(time_prefill_f16) / np.mean(time_prefill_i4_benchmark) if time_prefill_i4_benchmark is not None else None
+    print(f"Prefill time: {np.mean(time_prefill_i4):.3f} +- {1.96 * np.std(time_prefill_i4):.3f}ms\n"
+            f"Speedup: {prefill_speedup:.3f}"
+            + (f" Speedup loss: {prefill_benchmark_speedup - prefill_speedup:.3f}" if time_prefill_i4_benchmark is not None else ""))
+    if args.decode_steps is not None and args.decode_steps != 0:
+        decode_speedup = np.mean(time_decode_f16) / np.mean(time_decode_i4)
+        decode_benchmark_speedup = np.mean(time_decode_f16) / np.mean(time_decode_i4_benchmark) if time_decode_i4_benchmark is not None else None
+        print(f"Decode time: {np.mean(time_decode_i4):.3f} +- {1.96 * np.std(time_decode_i4):.3f}ms\n"
+                f"Speedup: {decode_speedup:.3f}"
+                + (f" Speedup loss: {decode_benchmark_speedup - decode_speedup:.3f}" if time_decode_i4_benchmark is not None else ""))
+        e2e_speedup = np.mean(time_e2e_f16) / np.mean(time_e2e_i4)
+        e2e_benchmark_speedup = np.mean(time_e2e_f16) / np.mean(time_e2e_i4_benchmark) if time_e2e_i4_benchmark is not None else None
+        print(f"E2E time: {np.mean(time_e2e_i4):.3f} +- {1.96 * np.std(time_e2e_i4):.3f}ms\n"
+                f"Speedup: {e2e_speedup:.3f}"
+                + (f" Speedup loss: {e2e_benchmark_speedup - e2e_speedup:.3f}" if time_e2e_i4_benchmark is not None else ""))
+
+## print allocated gpu memory
+def print_gpu_memory(prefix=""):
+    allocated = torch.cuda.memory_allocated() / (1024 ** 3)
+    print(f"{prefix} GPU memory - Allocated: {allocated:.2f} GB")
+    return allocated
+
+def benchmark(args):
+    for config_name in model_configs:
+        pprint.pprint(vars(args))
+
+        print("Loading dataset...")
+        test_data = load_dataset(config_name = config_name)
+        print(f"Loaded dataset")
+
+        # FP16
+        args.fuseLN, args.trans = False, "none"
+        args.online_trans = set()
+        model = get_model_fp16(config_name)
+        model.to('cuda')
+
+        if args.random_mode:
+            time_prefill_f16, time_decode_f16, time_e2e_f16, mem_f16 = run_all_for_model(
+                model, args.batch_size, args.prefill_seq_len, args.decode_steps)
+
+        print(f'------------------------- FP16 ------------------------')
+        ppl_f16, time_f16, std_f16 = ppl_eval(model = model, testenc = test_data)
+        del model
+        _cleanup()
+
+        if args.random_mode:
+            print(f"Prefill time: {np.mean(time_prefill_f16):.3f} +- {1.96 * np.std(time_prefill_f16):.3f}ms")
+            if args.decode_steps is not None and args.decode_steps != 0:
+                print(f"Decode time: {np.mean(time_decode_f16):.3f} +- {1.96 * np.std(time_decode_f16):.3f}ms")
+                print(f"E2E time: {np.mean(time_e2e_f16):.3f} +- {1.96 * np.std(time_e2e_f16):.3f}ms")
+
+        print(f"test-inference time: {time_f16:.3f} +- {1.96 * std_f16:.3f}ms per sequence")
+        print(f"Perplexity: {ppl_f16:.3f}")
+
+        # Int4
+        print(f'------------------------- Int4 ------------------------')
+        args.fuseLN, args.trans = False, "none"
+        args.online_trans = set()
+        model = get_model_quantized(args, config_name)
+
+        if args.random_mode:
+            time_prefill_i4_benchmark, time_decode_i4_benchmark, time_e2e_i4_benchmark, mem_i4 = run_all_for_model(
+                model, args.batch_size, args.prefill_seq_len, args.decode_steps)
+        
+        ppl_i4_benchmark, time_i4_benchmark, std_i4_benchmark = ppl_eval(model = model, testenc = test_data)
+        del model
+        _cleanup()
+
+        if args.random_mode:
+            print_e2e_time(args, time_prefill_i4_benchmark, time_decode_i4_benchmark, time_e2e_i4_benchmark,
+                        time_prefill_f16, time_decode_f16, time_e2e_f16)
+        
+        speedup_i4_benchmark = time_f16 / time_i4_benchmark
+        print(f"test-inference time: {time_i4_benchmark:.3f} +- {1.96 * std_i4_benchmark:.3f}ms per sequence")
+        print(f"Speedup: {speedup_i4_benchmark:.3f}x")
+        print(f"Perplexity: {ppl_i4_benchmark:.3f}")
+        print(f"Perplexity degradation: {ppl_i4_benchmark / ppl_f16:.3f}")
+
+        # QuaRot
+        args.fuseLN, args.trans = True, "had"
+        online_trans_list = [
+            {"o_proj", "qk", "down_proj"}
+        ]
+        for online_trans in online_trans_list:
+            print(f"------------------------- Int4 QuaRot ({'+'.join(online_trans)}) ------------------------")
+            args.online_trans = online_trans
+            model = get_model_quantized(args, config_name)
+
+            if args.random_mode:
+                time_prefill_i4, time_decode_i4, time_e2e_i4, mem_i4 = run_all_for_model(
+                    model, args.batch_size, args.prefill_seq_len, args.decode_steps)
+            
+            ppl_i4, time_i4, std_i4 = ppl_eval(model = model, testenc = test_data)
+            del model
+            _cleanup()
+
+            if args.random_mode:
+                print_e2e_time(args, time_prefill_i4, time_decode_i4, time_e2e_i4,
+                            time_prefill_f16, time_decode_f16, time_e2e_f16,
+                            time_prefill_i4_benchmark, time_decode_i4_benchmark, time_e2e_i4_benchmark)
+   
+        speedup_i4 = time_f16 / time_i4
+        print(f"test-inference time: {time_i4:.3f} +- {1.96 * std_i4:.3f}ms per sequence")
+        print(f"Speedup: {speedup_i4:.3f}x Speedup loss: {(speedup_i4_benchmark - speedup_i4):.3f}")
+        print(f"Perplexity: {ppl_i4:.3f}")
+        print(f"Perplexity degradation: {ppl_i4 / ppl_f16:.3f}")
+            
+        # FlatQuant
+        args.fuseLN, args.trans = False, "matmul"
+        online_trans_list = [
+            {"qk", "o_proj", "down_proj", "qkv_proj", "up_gate_proj"}
+        ]
+        for online_trans in online_trans_list:
+            print(f"------------------------- Int4 FlatQuant ({'+'.join(online_trans)}) ------------------------")
+            args.online_trans = online_trans
+            import os
+            model_name = os.path.basename(config_name)
+            weight_dir = os.path.join(".", "outputs", model_name, "w4a4", "exp")
+
+            model = get_model_quantized(args, config_name, weight_dir)
+
+            if args.random_mode:
+                time_prefill_i4, time_decode_i4, time_e2e_i4, mem_i4 = run_all_for_model(
+                    model, args.batch_size, args.prefill_seq_len, args.decode_steps)
+                        
+            ppl_i4, time_i4, std_i4 = ppl_eval(model = model, testenc = test_data)
+            del model
+            _cleanup()
+
+            if args.random_mode:
+                print_e2e_time(args, time_prefill_i4, time_decode_i4, time_e2e_i4,
+                            time_prefill_f16, time_decode_f16, time_e2e_f16,
+                            time_prefill_i4_benchmark, time_decode_i4_benchmark, time_e2e_i4_benchmark)
+            
+            speedup_i4 = time_f16 / time_i4
+            print(f"test-inference time: {time_i4:.3f} +- {1.96 * std_i4:.3f}ms per sequence")
+            print(f"Speedup: {speedup_i4:.3f}x Speedup loss: {(speedup_i4_benchmark - speedup_i4):.3f}")
+            print(f"Perplexity: {ppl_i4:.3f}")
+            print(f"Perplexity degradation: {ppl_i4 / ppl_f16:.3f}")
+
+if __name__ == '__main__':
+    parser = argparse.ArgumentParser()
+
+    parser.add_argument(
+        '--batch_size', type=int,
+        help='Batch size',
+        default=None,
+    )
+    parser.add_argument(
+        '--prefill_seq_len', type=int,
+        help='Size of the input sequence',
+        default=2048,
+    )
+    parser.add_argument(
+        '--decode_steps', type=int,
+        help='Decode steps',
+        default=256,
+    )
+    parser.add_argument(
+        '--random_mode', action = 'store_true',
+        help = 'Check speedup with random weight (original version)',
+        default = False, 
+    )
+    
+    args = parser.parse_args()
+    if args.batch_size is None:
+        for bsz in [1, 2, 4, 8, 16, 32, 64]:
+            args.batch_size = bsz
+            benchmark(args)
+    else:
+        benchmark(args)
\ No newline at end of file
diff --git a/benchmarks/layer_benchmark.py b/benchmarks/layer_benchmark.py
index 371ae0d..00b1784 100644
--- a/benchmarks/layer_benchmark.py
+++ b/benchmarks/layer_benchmark.py
@@ -13,9 +13,9 @@ import transformers
 
 
 model_configs = [
-    # "./modelzoo/llama-2-7b",
+    "./modelzoo/llama-2/llama-2-7b",
     # "./modelzoo/llama-2-13b",
-    "./modelzoo/llama-3/llama-3-8b",
+    #"./modelzoo/llama-3/llama-3-8b",
 ]
 
 benchmark_dtypes = ["int4", torch.float16]
diff --git a/deploy/functional/__init__.py b/deploy/functional/__init__.py
index 256c183..29951ec 100644
--- a/deploy/functional/__init__.py
+++ b/deploy/functional/__init__.py
@@ -1,3 +1,3 @@
 from .quantization import pack_i4, unpack_i4, asym_quant_dequant, sym_quant_dequant
 from .online_trans import (
-    matmul_hadU_cuda)
+    matmul_hadU_cuda, get_decompose_dim)
diff --git a/deploy/functional/online_trans.py b/deploy/functional/online_trans.py
index b25f70a..26e0a85 100644
--- a/deploy/functional/online_trans.py
+++ b/deploy/functional/online_trans.py
@@ -2,8 +2,21 @@ import torch, math
 import fast_hadamard_transform
 from deploy.kernels.kron_matmul import kron_matmul
 from deploy.kernels.block_matmul import block_matmul
+import deploy
 # Adapted from https://github.com/Cornell-RelaxML/quip-sharp/blob/main/lib/utils/matmul_had.py
 
+def get_decompose_dim(n):
+    a = int(math.sqrt(n))
+    if a * a < n:
+        a += 1
+    while True:
+        tmp = a*a - n
+        b = int(math.sqrt(tmp))
+        if b * b == tmp:
+            break
+        a += 1
+    return a - b, a + b
+
 
 def get_hadK(n, transpose=False):
     hadK, K = None, None
@@ -74,21 +87,53 @@ def get_hadK(n, transpose=False):
 #         raise NotImplementedError
 #     return x.reshape(init_shape)
 
+def quant(x, clip_factor_a_max = 1.0, clip_factor_a_min = 1.0, input_clip_ratio=1.0):
+    if clip_factor_a_max != 1.0:
+        reshaped_x = x.reshape((-1, x.shape[-1]))
+        xmax, xmin = reshaped_x.amax(1, keepdim=True), reshaped_x.amin(1, keepdim=True)
+        tmp = torch.zeros_like(xmax)
+        xmax, xmin = torch.maximum(xmax, tmp), torch.minimum(xmin, tmp)
+
+        xmax = xmax * torch.sigmoid(torch.tensor(clip_factor_a_max).to(x.device))
+        xmin = xmin * torch.sigmoid(torch.tensor(clip_factor_a_min).to(x.device))
+
+        xmax = torch.maximum(torch.abs(xmin), xmax)
+        tmp = xmax == 0
+        scales_x = (xmax / 7)
+        scales_x[tmp] = 1
+        scales_x = scales_x.to(torch.float16)
+    else:
+        scales_x = (torch.max(torch.abs(x), dim=-1)[0].unsqueeze(1)/7).to(torch.float16) * input_clip_ratio
 
-def kronecker_matmul(x, invs):
+    quantized_x = deploy.sym_quant(x, scales_x)
+    packed_tensor = deploy.PackedQuantizedTensor(quantized_x, scales_x)
+    return packed_tensor
+
+
+def kronecker_matmul(x, invs, clip_factor_a_max = 1.0, clip_factor_a_min = 1.0):
     init_shape = x.shape
     if len(invs) == 2:
         bsz, seq_len, hidden_dim = init_shape
         invL, invR = invs
+        invL = invL.T.contiguous()
         x = x.reshape(-1, invL.shape[0], invR.shape[0])
-        x = kron_matmul(invL, x, invR, seq_len)
+        x = kron_matmul(invL, x, invR, seq_len, clip_factor_a_max, clip_factor_a_min)
         x.quantized_x = x.quantized_x.reshape(bsz, seq_len, -1)
         x.scales_x = x.scales_x.reshape(bsz, 1, seq_len)
     elif len(invs) == 1:
         bsz, seq_len, head_dim, num_heads = init_shape
         inv = invs[0]
         x = x.reshape(-1, head_dim, num_heads)
-        x = block_matmul(x, inv, seq_len)
+        (x @ inv).contiguous()
+        just_quantize = not ((head_dim > 0 and math.log2(head_dim).is_integer()) and (num_heads > 0 and math.log2(num_heads).is_integer()))
+        
+        x = block_matmul(x, inv, seq_len, clip_factor_a_max, clip_factor_a_min, just_quantize)
+        # TODO: kernel only support for dim == power of 2
+        if just_quantize:
+            x = x.reshape(bsz, seq_len, head_dim, -1)
+            x = x.transpose(-1,-2).contiguous().reshape(bsz, seq_len, -1)
+            x = quant(x, clip_factor_a_max, clip_factor_a_min)
+
         x.quantized_x = x.quantized_x.reshape(bsz, seq_len, -1, num_heads)
         x.scales_x = x.scales_x.reshape(bsz, 1, seq_len)
     else:
diff --git a/deploy/kernels/block_matmul.py b/deploy/kernels/block_matmul.py
index 75e7670..54ba7d8 100644
--- a/deploy/kernels/block_matmul.py
+++ b/deploy/kernels/block_matmul.py
@@ -39,6 +39,8 @@ def matmul_quant_kernel(
         stride_bb, stride_bk, stride_bn,  
         stride_ck, stride_cn,
         stride_resb, stride_resm, stride_resn,
+        clip_factor_a_max,
+        clip_factor_a_min,
         BLOCK_SIZE_K: tl.constexpr,
 ):
     """
@@ -66,12 +68,25 @@ def matmul_quant_kernel(
         b_ptrs += BLOCK_SIZE_K * stride_bn
         c_ptrs += BLOCK_SIZE_K * stride_ck
 
-    abs_src_val = tl.abs(accumulator)
-    max_src_val = tl.max(abs_src_val)
+    xmax = tl.max(accumulator)
+    xmin = tl.min(accumulator)
+    sigmoid_max = 1.0 / (1.0 + tl.exp(-clip_factor_a_max))
+    sigmoid_min = 1.0 / (1.0 + tl.exp(-clip_factor_a_min))
+
+    xmax = xmax * sigmoid_max
+    xmin = xmin * sigmoid_min
+
+    abs_xmin = tl.abs(xmin)
+    max_src_val = tl.maximum(abs_xmin, xmax)
 
     scale = max_src_val / 7.
-    quant_val = libdevice.llrint(accumulator / scale)
-    quant_val = max(-8, min(quant_val, 7))
+
+    scale = tl.where(scale == 0.0, 1.0, scale)
+
+    accumulator_T = tl.trans(accumulator)
+    quant_val = libdevice.llrint(accumulator_T / scale)
+    quant_val = tl.maximum(-8, tl.minimum(quant_val, 7))
+
 
     quant_val = quant_val.reshape(np2_M, np2_N // 2, 2, can_reorder=False)
     quant_val_even, quant_val_odd = quant_val.split()
@@ -178,12 +193,24 @@ def quant_kernel(
     src_mask = (index_rows[:, None] < M) & (index_cols[None, :] < N)
     src = tl.load(src_ptrs, mask=src_mask, other=0.0)
 
-    abs_src_val = tl.abs(src)
-    max_src_val = tl.max(abs_src_val)
+    xmax = tl.max(src)
+    xmin = tl.min(src)
+    sigmoid_max = 1.0 / (1.0 + tl.exp(-clip_factor_a_max))
+    sigmoid_min = 1.0 / (1.0 + tl.exp(-clip_factor_a_min))
+
+    xmax = xmax * sigmoid_max
+    xmin = xmin * sigmoid_min
+
+    abs_xmin = tl.abs(xmin)
+    max_src_val = tl.maximum(abs_xmin, xmax)
+
     scale = max_src_val / 7.
+    
+    scale = tl.where(scale == 0.0, 1.0, scale)
 
-    quant_val = libdevice.llrint(src / scale)
-    quant_val = max(-8, min(quant_val, 7))
+    src_T = tl.trans(src)
+    quant_val = libdevice.llrint(src_T / scale)
+    quant_val = tl.maximum(-8, tl.minimum(quant_val, 7))
     quant_val = quant_val.reshape(np2_M,  np2_N // 2, 2, can_reorder=False)
     quant_val_even, quant_val_odd = quant_val.split()
     quant_val_odd = quant_val_odd << 4
@@ -201,7 +228,7 @@ def quant_kernel(
 
 
 FUSION=True
-def block_matmul(b, c, seq_len):
+def block_matmul(b, c, seq_len, clip_factor_a_max, clip_factor_a_min, just_quantize = False):
     # Check constraints.
     # b @ c, b [b, m, n], c [n, n]
     assert b.shape[2] == c.shape[0], "Incompatible dimensions"
@@ -214,21 +241,12 @@ def block_matmul(b, c, seq_len):
     output_scale = torch.empty((B, 1), device=b.device, dtype=torch.float16)
     quant_res = torch.empty((B, M, N // 2), device=b.device, dtype=torch.uint8)
 
+    # TODO: handle shared memory issue. In this setting, A100 can't get full speedup.
+    np2_M = min(triton.next_power_of_2(M), 128)
+    np2_N = min(triton.next_power_of_2(N), 32)
+
     # 1D launch kernel where each block gets its own program.
-    if FUSION:
-        grid = (1, seq_len, Actual_B)
-        matmul_quant_kernel[grid](
-            b, c,  #
-            quant_res, #
-            output_scale, #
-            B, M, N,  #
-            triton.next_power_of_2(M),
-            triton.next_power_of_2(N),
-            b.stride(0), b.stride(1), b.stride(2),  #
-            c.stride(0), c.stride(1), #
-            quant_res.stride(0), quant_res.stride(1), quant_res.stride(2),  #
-        )
-    else:
+    if just_quantize:
         bmm_res = torch.empty((B, M, N), device=b.device, dtype=b.dtype)
         grid = (1, seq_len, Actual_B)
         matmul_kernel[grid](
@@ -236,25 +254,58 @@ def block_matmul(b, c, seq_len):
             bmm_res, #
             output_scale, #
             B, M, N,  #
-            triton.next_power_of_2(M),
-            triton.next_power_of_2(N),
+            np2_M,
+            np2_N,
             b.stride(0), b.stride(1), b.stride(2),  #
             c.stride(0), c.stride(1), #
             bmm_res.stride(0), bmm_res.stride(1), bmm_res.stride(2),  #
         )
-        grid = (seq_len, Actual_B)
-        quant_kernel[grid](
-            bmm_res,
-            bmm_res.stride(0), bmm_res.stride(1), bmm_res.stride(2), 
-            quant_res,
-            quant_res.stride(0), quant_res.stride(1), quant_res.stride(2),
-            output_scale,
-            B, M, N,
-            triton.next_power_of_2(M),
-            triton.next_power_of_2(N),
-        )
-    packed_tensor = deploy.PackedQuantizedTensor(quant_res.reshape(B, -1), output_scale)
-    return packed_tensor
+        return bmm_res.view(B, -1)
+    else:
+        if FUSION:
+            grid = (1, seq_len, Actual_B)
+            matmul_quant_kernel[grid](
+                b, c,  #
+                quant_res, #
+                output_scale, #
+                B, M, N,  #
+                np2_M,
+                np2_N,
+                b.stride(0), b.stride(1), b.stride(2),  #
+                c.stride(0), c.stride(1), #
+                quant_res.stride(0), quant_res.stride(1), quant_res.stride(2),  #
+                clip_factor_a_max,
+                clip_factor_a_min,
+            )
+        else:
+            bmm_res = torch.empty((B, M, N), device=b.device, dtype=b.dtype)
+            grid = (1, seq_len, Actual_B)
+            matmul_kernel[grid](
+                b, c,  #
+                bmm_res, #
+                output_scale, #
+                B, M, N,  #
+                np2_M,
+                np2_N,
+                b.stride(0), b.stride(1), b.stride(2),  #
+                c.stride(0), c.stride(1), #
+                bmm_res.stride(0), bmm_res.stride(1), bmm_res.stride(2),  #
+            )
+            grid = (seq_len, Actual_B)
+            quant_kernel[grid](
+                bmm_res,
+                bmm_res.stride(0), bmm_res.stride(1), bmm_res.stride(2), 
+                quant_res,
+                quant_res.stride(0), quant_res.stride(1), quant_res.stride(2),
+                output_scale,
+                B, M, N,
+                triton.next_power_of_2(M),
+                triton.next_power_of_2(N),
+                clip_factor_a_max,
+                clip_factor_a_min,
+            )
+        packed_tensor = deploy.PackedQuantizedTensor(quant_res.reshape(B, -1), output_scale)
+        return packed_tensor
 
 
 def benchmark(B, M, N, S, provider):
@@ -268,5 +319,5 @@ def benchmark(B, M, N, S, provider):
     if provider == 'triton':
         ms, min_ms, max_ms = triton.testing.do_bench(lambda: block_matmul(b, c, S), quantiles=quantiles)
     perf = lambda ms: 2 * B * M * N * N * 1e-12 / (ms * 1e-3)
-    import pdb; pdb.set_trace()
+    #import pdb; pdb.set_trace()
     return perf(ms), perf(max_ms), perf(min_ms), ms, max_ms, min_ms
diff --git a/deploy/kernels/kron_matmul.py b/deploy/kernels/kron_matmul.py
index 12bbefa..ee25272 100644
--- a/deploy/kernels/kron_matmul.py
+++ b/deploy/kernels/kron_matmul.py
@@ -37,6 +37,8 @@ def matmul_kernel(
         stride_resb, stride_resm, stride_resn,
         BLOCK_SIZE_M: tl.constexpr, # we use BLOCK_SIZE_M == triton.next_power_of_2(BLOCK_SIZE_M) to fuse quant into matmul
         is_split: tl.constexpr,
+        clip_factor_a_max,
+        clip_factor_a_min,
 ):
     """
     a @ b @ c
@@ -86,12 +88,23 @@ def matmul_kernel(
         # atomic max does support fp16
         # tl.atomic_max(output_scale + batch_id, max_src_val.to(tl.float16))
     else:
-        abs_src_val = tl.abs(accumulator)
-        max_src_val = tl.max(abs_src_val)
+        xmax = tl.max(accumulator)
+        xmin = tl.min(accumulator)
+        sigmoid_max = 1.0 / (1.0 + tl.exp(-clip_factor_a_max))
+        sigmoid_min = 1.0 / (1.0 + tl.exp(-clip_factor_a_min))
+
+        xmax = xmax * sigmoid_max
+        xmin = xmin * sigmoid_min
+
+        abs_xmin = tl.abs(xmin)
+        max_src_val = tl.maximum(abs_xmin, xmax)
 
         scale = max_src_val / 7.
+
+        scale = tl.where(max_src_val == 0.0, 1.0, scale)
+        
         quant_val = libdevice.llrint(accumulator / scale)
-        quant_val = max(-8, min(quant_val, 7))
+        quant_val = tl.maximum(-8, tl.minimum(quant_val, 7))
 
         quant_val = quant_val.reshape(BLOCK_SIZE_M, np2_N // 2, 2, can_reorder=False)
         quant_val_even, quant_val_odd = quant_val.split()
@@ -129,6 +142,8 @@ def quant_kernel(
         N: tl.constexpr,
         np2_M: tl.constexpr, 
         np2_N: tl.constexpr,
+        clip_factor_a_max,
+        clip_factor_a_min,
 ):
     '''
     quant fp16 tensor to int4
@@ -141,12 +156,23 @@ def quant_kernel(
     src_mask = (index_rows[:, None] < M) & (index_cols[None, :] < N)
     src = tl.load(src_ptrs, mask=src_mask, other=0.0)
 
-    abs_src_val = tl.abs(src)
-    max_src_val = tl.max(abs_src_val)
+    xmax = tl.max(src)
+    xmin = tl.min(src)
+    sigmoid_max = 1.0 / (1.0 + tl.exp(-clip_factor_a_max))
+    sigmoid_min = 1.0 / (1.0 + tl.exp(-clip_factor_a_min))
+
+    xmax = xmax * sigmoid_max
+    xmin = xmin * sigmoid_min
+
+    abs_xmin = tl.abs(xmin)
+    max_src_val = tl.maximum(abs_xmin, xmax)
+
     scale = max_src_val / 7.
 
+    scale = tl.where(scale == 0.0, 1.0, scale)
+
     quant_val = libdevice.llrint(src / scale)
-    quant_val = max(-8, min(quant_val, 7))
+    quant_val = tl.maximum(-8, tl.minimum(quant_val, 7))
     quant_val = quant_val.reshape(np2_M,  np2_N // 2, 2, can_reorder=False)
     quant_val_even, quant_val_odd = quant_val.split()
     quant_val_odd = quant_val_odd << 4
@@ -163,7 +189,7 @@ def quant_kernel(
     tl.store(output_scale + batch_id, scale)
 
 
-def kron_matmul(a, b, c, seq_len):
+def kron_matmul(a, b, c, seq_len, clip_factor_a_max, clip_factor_a_min):
     # Check constraints.
     # a @ b @ c, a [m, m], b [b, m, n], c [n, n]
     assert a.shape[1] == b.shape[1], "Incompatible dimensions"
@@ -180,6 +206,7 @@ def kron_matmul(a, b, c, seq_len):
     # is_split = True
     output_scale = torch.empty((B, 1), device=a.device, dtype=torch.float16)
     quant_res = torch.empty((B, M, N // 2), device=a.device, dtype=torch.uint8)
+    
     if is_split:
         bmm_res = torch.empty((B, M, N), device=a.device, dtype=a.dtype)
         # 2 x bmm
@@ -198,6 +225,8 @@ def kron_matmul(a, b, c, seq_len):
             bmm_res.stride(0), bmm_res.stride(1), bmm_res.stride(2),  #
             BLOCK_SIZE_M,
             is_split,
+            clip_factor_a_max = 1.0,
+            clip_factor_a_min = 1.0,
         )
         # quant fp16 to int4
         grid = (seq_len, Actual_B)
@@ -210,6 +239,8 @@ def kron_matmul(a, b, c, seq_len):
             B, M, N,
             triton.next_power_of_2(M),
             triton.next_power_of_2(N),
+            clip_factor_a_max,
+            clip_factor_a_min,
         )
         packed_tensor = deploy.PackedQuantizedTensor(quant_res.reshape(B, -1), output_scale)
     else:
@@ -228,6 +259,8 @@ def kron_matmul(a, b, c, seq_len):
             quant_res.stride(0), quant_res.stride(1), quant_res.stride(2),  #
             BLOCK_SIZE_M,
             is_split,
+            clip_factor_a_max,
+            clip_factor_a_min,
         )
         packed_tensor = deploy.PackedQuantizedTensor(quant_res.reshape(B, -1), output_scale)
     return packed_tensor
diff --git a/deploy/kernels/quant.cu b/deploy/kernels/quant.cu
index 84399d2..a62baf8 100644
--- a/deploy/kernels/quant.cu
+++ b/deploy/kernels/quant.cu
@@ -4,7 +4,9 @@
 template<typename T>
 __device__ __half int_to_half(T value)
 {
-    return __int2half_rn(static_cast<int>(value));
+    int int_value = static_cast<int>(value);
+    int_value = max(-65176, min(65176, int_value));
+    return __int2half_rn(int_value);
 }
 
 
diff --git a/deploy/nn/online_trans.py b/deploy/nn/online_trans.py
index 51baa3a..8d08c19 100644
--- a/deploy/nn/online_trans.py
+++ b/deploy/nn/online_trans.py
@@ -16,7 +16,7 @@ def get_decompose_dim(n):
 
 
 class OnlineTrans(torch.nn.Module):
-    def __init__(self, trans_dim, force_fp32=False, trans="had", decompose=True):
+    def __init__(self, trans_dim, force_fp32=False, trans="had", decompose=True, lac = False):
         super().__init__()
         self.fp32_trans = force_fp32
         self.trans = trans
@@ -37,6 +37,7 @@ class OnlineTrans(torch.nn.Module):
                 right_matrix = torch.randn([right_size, right_size], dtype=torch.float16)
                 self.register_buffer("left_matrix", left_matrix)
                 self.register_buffer("right_matrix", right_matrix)
+                self.register_buffer("diag_scale", torch.randn([trans_dim], dtype=torch.float16))
             else:
                 right_matrix = torch.randn([trans_dim, trans_dim], dtype=torch.float16)
                 self.register_buffer("right_matrix", right_matrix)
@@ -47,6 +48,10 @@ class OnlineTrans(torch.nn.Module):
             # else:
             #     self.right_matrix = torch.nn.Parameter(torch.randn([trans_dim, trans_dim], dtype=torch.float16), requires_grad=True)
 
+        self.lac = lac
+        self.register_buffer("clip_factor_a_max", torch.tensor(1.0))
+        self.register_buffer("clip_factor_a_min", torch.tensor(1.0))
+
     def forward(self, x):
         if self.fp32_trans:
             x = x.float()
@@ -58,5 +63,5 @@ class OnlineTrans(torch.nn.Module):
                 invs.append(self.left_matrix)
             if hasattr(self, "right_matrix"):
                 invs.append(self.right_matrix)
-            x = deploy.functional.online_trans.kronecker_matmul(x, invs)
+            x = deploy.functional.online_trans.kronecker_matmul(x, invs, self.clip_factor_a_max, self.clip_factor_a_min)
         return x
diff --git a/deploy/nn/quantization.py b/deploy/nn/quantization.py
index 85c0276..31e6a49 100644
--- a/deploy/nn/quantization.py
+++ b/deploy/nn/quantization.py
@@ -3,13 +3,32 @@ import torch
 
 
 class Quantizer(torch.nn.Module):
-    def __init__(self, input_clip_ratio=1.0):
+    def __init__(self, input_clip_ratio=1.0, lac = False):
         super().__init__()
         self.input_clip_ratio = input_clip_ratio
-    
+        self.lac = lac
+        self.register_buffer("clip_factor_a_max", torch.tensor(4.0))
+        self.register_buffer("clip_factor_a_min", torch.tensor(4.0))
+
     def forward(self, x):
         if not isinstance(x, deploy.PackedQuantizedTensor):
-            scales_x = (torch.max(torch.abs(x), dim=-1)[0].unsqueeze(1)/7).to(torch.float16) * self.input_clip_ratio
+            if self.lac:
+                reshaped_x = x.reshape((-1, x.shape[-1]))
+                xmax, xmin = reshaped_x.amax(1, keepdim=True), reshaped_x.amin(1, keepdim=True)
+                tmp = torch.zeros_like(xmax)
+                xmax, xmin = torch.maximum(xmax, tmp), torch.minimum(xmin, tmp)
+
+                xmax = xmax * torch.sigmoid(self.clip_factor_a_max.to(x.device))
+                xmin = xmin * torch.sigmoid(self.clip_factor_a_min.to(x.device))
+
+                xmax = torch.maximum(torch.abs(xmin), xmax)
+                tmp = xmax == 0
+                scales_x = (xmax / 7)
+                scales_x[tmp] = 1
+                scales_x = scales_x.to(torch.float16)
+            else:
+                scales_x = (torch.max(torch.abs(x), dim=-1)[0].unsqueeze(1)/7).to(torch.float16) * self.input_clip_ratio
+
             quantized_x = deploy.sym_quant(x, scales_x)
             packed_tensor = deploy.PackedQuantizedTensor(quantized_x, scales_x)
             return packed_tensor
diff --git a/deploy/transformers/kv_cache.py b/deploy/transformers/kv_cache.py
index 8d39dcf..657b216 100644
--- a/deploy/transformers/kv_cache.py
+++ b/deploy/transformers/kv_cache.py
@@ -9,13 +9,41 @@ from deploy.functional.quantization import get_minq_maxq
 
 
 @torch.jit.script
-def asym_quantize_and_pack_i4(x: torch.Tensor):
+def asym_quantize_and_pack_i4(x: torch.Tensor, clip_factor_a_max: torch.Tensor, clip_factor_a_min: torch.Tensor, lac: bool = False, quantize: bool = True):
     minq, maxq = get_minq_maxq(bits=4, sym=False)
     xmax = torch.amax(x, dim=-1, keepdim=True)
     xmin = torch.amin(x, dim=-1, keepdim=True)
-    scale = ((xmax - xmin).clamp(min=1e-5) / maxq)
-    zero = -xmin
-    q = torch.clamp(torch.round((x + zero) / scale), 0, maxq)
+
+    if lac:
+        tmp = torch.zeros_like(xmax)
+        xmax, xmin = torch.maximum(xmax, tmp), torch.minimum(xmin, tmp)
+        xmax = xmax * clip_factor_a_max
+        xmin = xmin * clip_factor_a_min
+
+        xmin_zero = torch.eq(xmin, 0.0)
+        xmax_zero = torch.eq(xmax, 0.0)
+        zero_mask = xmin_zero * xmax_zero
+        
+        neg_one = torch.full_like(xmin, -1.0)
+        pos_one = torch.full_like(xmax, 1.0)
+        
+        xmin = torch.where(zero_mask, neg_one, xmin)
+        xmax = torch.where(zero_mask, pos_one, xmax)
+
+        scale = (xmax - xmin) / maxq
+        zero = torch.round((-1.0 * xmin) / scale)
+        q = torch.clamp((x / scale).round() + zero, 0, maxq)
+
+        if not quantize:
+            return scale * (q - zero), scale, zero
+
+    else:
+        scale = ((xmax - xmin).clamp(min=1e-5) / maxq)
+        zero = -xmin
+        q = torch.clamp(torch.round((x + zero) / scale), 0, maxq)
+
+        if not quantize:
+            return scale * (q - zero), scale, zero
 
     # pack int4
     q = q.to(dtype=torch.uint8)
@@ -23,11 +51,14 @@ def asym_quantize_and_pack_i4(x: torch.Tensor):
     return q, scale, zero
 
 
-def unpack_i4_and_asym_dequantize(q, scale, zero):
+def unpack_i4_and_asym_dequantize(q, scale, zero, lac: bool = False):
     #unpack int4
     assert q.dtype == torch.uint8
     q = torch.stack((q & 0x0f, (q >> 4) & 0x0f), dim=-1).view(*q.shape[:-1], q.shape[-1] * 2)
-    return q * scale - zero
+    if lac:
+        return scale * (q - zero)
+    else:
+        return q * scale - zero
 
 
 def matmul_had_cuda(X, dtype):
@@ -141,7 +172,7 @@ class MultiLayerPagedKVCache4Bit(Cache):
         self, batch_size, page_size, max_seq_len, 
         device, n_layers, num_heads, head_dim, 
         disable_quant=False, trans_dtype=torch.float16,
-        trans="had"):
+        trans="had", group_size = 1):
         self.page_size = page_size
         self.batch_size = batch_size
         max_page_cnt = self.page_cnt_from_length(max_seq_len)
@@ -156,15 +187,9 @@ class MultiLayerPagedKVCache4Bit(Cache):
                 head_dim if disable_quant else head_dim // 2 
             ), 
             dtype=torch.float16 if disable_quant else torch.uint8, device=device)
-        self.trans = trans
-        if self.trans == "had":
-            self.head_dim = None
-        elif self.trans.startswith("matmul"):
-            self.head_dim = torch.randn([head_dim, head_dim], requires_grad=False).to(trans_dtype).to(device)
-        else:
-            trans_dtype = None
-            self.head_dim = None
         
+        self.org_head_dim = head_dim
+        self.trans = trans
         self.scales = torch.empty((max_page_cnt * batch_size, n_layers, 2, num_heads, page_size,  2), dtype=torch.float16, device=device)
         self.page_size = page_size
         self.max_seq_len = max_seq_len
@@ -172,11 +197,8 @@ class MultiLayerPagedKVCache4Bit(Cache):
         self.length = 0
         self.device = device
         self.trans_dtype = trans_dtype
-        self._stub = _AttentionStub(
-            self.page_size, device, n_layers, 
-            disable_quant=self.disable_quant, 
-            trans_dtype=self.trans_dtype,
-            head_dim=self.head_dim)
+        self.n_layers = n_layers
+        self.group_size = group_size
 
     def page_cnt_from_length(self, length):
         return (length + self.page_size - 1) // self.page_size
@@ -199,28 +221,67 @@ class MultiLayerPagedKVCache4Bit(Cache):
         cache_kwargs: Optional[Dict[str, Any]] = None,
     ):
         
+        self.trans_matrix_k = cache_kwargs.get("trans_matrix_k")
+        self.trans_matrix_k_inv_t = cache_kwargs.get("trans_matrix_k_inv_t")
+        self.trans_matrix_v = cache_kwargs.get("trans_matrix_v")
+        self.kclip_factor_a_max = cache_kwargs.get("kclip_factor_a_max")
+        self.kclip_factor_a_min = cache_kwargs.get("kclip_factor_a_min")
+        self.vclip_factor_a_max = cache_kwargs.get("vclip_factor_a_max")
+        self.vclip_factor_a_min = cache_kwargs.get("vclip_factor_a_min")
+        
+        if self.trans == "had":
+            self.head_dim = None
+        elif self.trans.startswith("matmul"):
+            self.head_dim = torch.randn([self.org_head_dim, self.org_head_dim], requires_grad=False).to(self.trans_dtype).to(self.device)
+            self.head_dim = self.trans_matrix_k
+            self.head_dim_inv_t = self.trans_matrix_k_inv_t
+        else:
+            self.trans_dtype = None
+            self.head_dim = None
+            
+        self._stub = _AttentionStub(
+            self.page_size, self.device, self.n_layers, 
+            disable_quant=self.disable_quant, 
+            trans_dtype=self.trans_dtype,
+            head_dim = self.head_dim_inv_t if self.trans.startswith("matmul") else self.head_dim) # isFlatQ
+        
         b_sz, added_length, num_heads, head_dim = key_states.shape
 
         orig_key_states = key_states
         orig_value_states = value_states
-
         if self.trans_dtype is not None:
             if self.head_dim is None:
                 key_states = matmul_had_cuda(key_states, dtype=self.trans_dtype)
             else:
-                key_states = torch.matmul(key_states.to(self.trans_dtype), self.head_dim)
-
+                key_states = torch.matmul(key_states.to(self.trans_dtype), self.head_dim) ## trans for k
         if self.disable_quant:
             k_scale = key_states.new_ones((b_sz, added_length, num_heads, 1))
             k_zero = key_states.new_zeros((b_sz, added_length, num_heads, 1))
             v_scale = value_states.new_ones((b_sz, added_length, num_heads, 1))
             v_zero = value_states.new_zeros((b_sz, added_length, num_heads, 1))
         else:
-            key_states, k_scale, k_zero = asym_quantize_and_pack_i4(key_states)
-            value_states, v_scale, v_zero = asym_quantize_and_pack_i4(value_states)
+            self.kclip_factor_a_max, self.kclip_factor_a_min = torch.sigmoid(self.kclip_factor_a_max), torch.sigmoid(self.kclip_factor_a_min)
+            self.vclip_factor_a_max, self.vclip_factor_a_min = torch.sigmoid(self.vclip_factor_a_max), torch.sigmoid(self.vclip_factor_a_min)
+            if self.trans.startswith("matmul"):
+                self.lac = True
+            else:
+                self.lac = False
+            orig_key_states = key_states
+            orig_value_states = value_states
+            # key_states_lac_only, _, _ = asym_quantize_and_pack_i4(key_states, clip_factor_a_max = self.kclip_factor_a_max, clip_factor_a_min = self.kclip_factor_a_min, lac = self.lac, quantize = False) # if we wants to use lac for prefill kv, use it
+            # value_states_lac_only, _, _ = asym_quantize_and_pack_i4(value_states, clip_factor_a_max = self.vclip_factor_a_max, clip_factor_a_min = self.vclip_factor_a_min, lac = self.lac, quantize = False)
+            key_states, k_scale, k_zero = asym_quantize_and_pack_i4(key_states, clip_factor_a_max = self.kclip_factor_a_max, clip_factor_a_min = self.kclip_factor_a_min) # lac = false / if we wnats to use lac for cached kv, use lac = True
+            value_states, v_scale, v_zero = asym_quantize_and_pack_i4(value_states, clip_factor_a_max = self.vclip_factor_a_max, clip_factor_a_min = self.vclip_factor_a_min)
         
-        k_param = torch.cat([k_scale, k_zero], dim=-1).view(self.batch_size * added_length, num_heads, 2)
-        v_param = torch.cat([v_scale, v_zero], dim=-1).view(self.batch_size * added_length, num_heads, 2)     
+        if self.group_size > 1:
+            # Repeat the quantized states
+            key_states = key_states.repeat_interleave(self.group_size, dim = 2)
+            value_states = value_states.repeat_interleave(self.group_size, dim = 2)
+            # Update num_heads to match after repeat
+            num_heads = num_heads * self.group_size
+        
+        k_param = torch.cat([k_scale, k_zero], dim=-1).repeat_interleave(self.group_size, dim = -2).view(self.batch_size * added_length, num_heads, 2)
+        v_param = torch.cat([v_scale, v_zero], dim=-1).repeat_interleave(self.group_size, dim = -2).view(self.batch_size * added_length, num_heads, 2)
 
         quantized_head_dim = self.pages.shape[-1]
 
@@ -259,7 +320,11 @@ class MultiLayerPagedKVCache4Bit(Cache):
                 seqlen_indptr=seqlens_in_batch,
                 layer_idx=layer_idx
             )
-            return orig_key_states, orig_value_states
+            
+            if key_states.dtype == torch.uint8 and self.trans.startswith("matmul"):
+                return orig_key_states, orig_value_states # key_states_lac_only, value_states_lac_only # if we wants to use lac for prefill kv, use it
+            else:
+                return orig_key_states, orig_value_states
         else:
             assert added_length == 1
             append_kv = append_kv_f16 if self.disable_quant else append_kv_i4
diff --git a/deploy/transformers/modeling_llama.py b/deploy/transformers/modeling_llama.py
index f9ba9aa..faa143d 100644
--- a/deploy/transformers/modeling_llama.py
+++ b/deploy/transformers/modeling_llama.py
@@ -1,6 +1,15 @@
+import sys
+import os
+
+current_dir = os.path.dirname(os.path.abspath(__file__))
+if current_dir not in sys.path:
+    sys.path.insert(0, current_dir)
+
+
 import functools
 import deploy
 import deploy.transformers
+from deploy.functional import get_decompose_dim
 import torch
 from transformers import LlamaConfig
 from transformers.models.llama.modeling_llama import LlamaAttention, \
@@ -21,8 +30,12 @@ class FlatQuantFP16LlamaAttention(LlamaFlashAttention2):
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
-        self.quantizer = torch.nn.Identity()
-        self.inp_trans = torch.nn.Identity()
+        self.quantizer_q = torch.nn.Identity()
+        self.quantizer_k = torch.nn.Identity()
+        self.quantizer_v = torch.nn.Identity()
+        self.inp_trans_q = torch.nn.Identity()
+        self.inp_trans_k = torch.nn.Identity()
+        self.inp_trans_v = torch.nn.Identity()
         self.o_proj_trans = torch.nn.Identity()
 
     def forward(
@@ -34,22 +47,42 @@ class FlatQuantFP16LlamaAttention(LlamaFlashAttention2):
         output_attentions: bool = False,
         use_cache: bool = False,
         cache_position: Optional[torch.LongTensor] = None,
+        cache_kwargs = None,
         **kwargs,
     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
         output_attentions = False
 
         bsz, q_len, _ = hidden_states.size()
+        
+        self.isFlatQ = False
+        if cache_kwargs is not None:
+            self.isFlatQ = cache_kwargs.get("isFlatQ", False)
+
+        if self.isFlatQ:
+            #hidden_states = self.inp_trans(hidden_states)
+            hidden_states_q = self.inp_trans_q(hidden_states)
+            hidden_states_k = self.inp_trans_k(hidden_states)
+            hidden_states_v = self.inp_trans_v(hidden_states)
+
+            hidden_states_q = self.quantizer_q(hidden_states_q)
+            hidden_states_k = self.quantizer_k(hidden_states_k)
+            hidden_states_v = self.quantizer_v(hidden_states_v)
+            
+            query_states = self.q_proj(hidden_states_q)
+            key_states = self.k_proj(hidden_states_k)
+            value_states = self.v_proj(hidden_states_v)
+        else:
+            hidden_states = self.inp_trans_q(hidden_states)
+            hidden_states = self.quantizer_q(hidden_states)
 
-        hidden_states = self.inp_trans(hidden_states)
-        hidden_states = self.quantizer(hidden_states)
-
-        query_states = self.q_proj(hidden_states)
-        key_states = self.k_proj(hidden_states)
-        value_states = self.v_proj(hidden_states)
-
+            query_states = self.q_proj(hidden_states)
+            key_states = self.k_proj(hidden_states)
+            value_states = self.v_proj(hidden_states)
+            
         # Flash attention requires the input to have the shape
         # batch_size x seq_length x head_dim x hidden_dim
         # therefore we just need to keep the original shape
+        group_size = self.num_heads // self.num_key_value_heads
         query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim)
         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim)
         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim)
@@ -62,10 +95,15 @@ class FlatQuantFP16LlamaAttention(LlamaFlashAttention2):
         past_key_value = getattr(self, "past_key_value", past_key_value)
         assert past_key_value is not None
         # sin and cos are specific to RoPE models; position_ids needed for the static cache
-        
-        cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position, "attention_mask": attention_mask}
+        if cache_kwargs is None:
+            cache_kwargs = {}
+        cache_kwargs.update({
+            "sin": sin,
+            "cos": cos,
+            "cache_position": cache_position,
+            "attention_mask": attention_mask,
+        })
         cache_out = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
-        
 
         dropout_rate = self.attention_dropout if self.training else 0.0
 
@@ -73,6 +111,9 @@ class FlatQuantFP16LlamaAttention(LlamaFlashAttention2):
 
         if isinstance(cache_out, tuple):
             key_states, value_states = cache_out
+            trans_mat_for_q = cache_kwargs.get("trans_matrix_k_inv_t")
+            if trans_mat_for_q is not None:
+                query_states = torch.matmul(query_states.to(torch.float16), trans_mat_for_q) # trans for q TODO: fix hard-coded trans dtype
             attn_output = self._flash_attention_forward(
                 query_states, 
                 key_states, 
@@ -86,8 +127,7 @@ class FlatQuantFP16LlamaAttention(LlamaFlashAttention2):
         if isinstance(self.o_proj_trans, deploy.nn.OnlineTrans) and self.o_proj_trans.trans == "matmul":
             # attn_output: (bsz, seq_len, num_heads, head_dim)
             attn_output = self.o_proj_trans(attn_output.transpose(-1, -2).contiguous())
-            attn_output.quantized_x = attn_output.quantized_x.transpose(-1, -2)
-            attn_output.quantized_x = attn_output.quantized_x.reshape(bsz, q_len, -1).contiguous()
+            attn_output.quantized_x = attn_output.quantized_x.contiguous().reshape(bsz, q_len, -1)
         else:
             attn_output = self.o_proj_trans(attn_output.transpose(-1, -2)).transpose(-1, -2)
             attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()
@@ -103,49 +143,125 @@ class FlatQuantLlamaAttention(FlatQuantFP16LlamaAttention):
 
     def __init__(self, options, *args, **kwargs):
         super().__init__(*args, **kwargs)
+        self.isFlatQ = hasattr(options, 'trans') and options.trans == "matmul" # if doing FlatQuant
+
         self.options = options
-        self.quantizer = deploy.nn.Quantizer()
+        self.quantizer_q = deploy.nn.Quantizer(lac = self.isFlatQ)
+        self.quantizer_k = deploy.nn.Quantizer(lac = self.isFlatQ)
+        self.quantizer_v = deploy.nn.Quantizer(lac = self.isFlatQ)
         self.q_proj = deploy.nn.Linear4bit.from_float(self.q_proj)
         self.k_proj = deploy.nn.Linear4bit.from_float(self.k_proj)
         self.v_proj = deploy.nn.Linear4bit.from_float(self.v_proj)
         if "o_proj" in self.options.online_trans:
             self.o_proj_trans = deploy.nn.OnlineTrans(self.num_heads, trans=options.trans, decompose=False)
         self.o_proj = torch.nn.Sequential(
-            deploy.nn.Quantizer(),
+            deploy.nn.Quantizer(lac = self.isFlatQ),
             deploy.nn.Linear4bit.from_float(self.o_proj)
         )
         if "qkv_proj" in self.options.online_trans:
             if not self.options.fuseLN:
-                self.inp_trans = deploy.nn.OnlineTrans(self.hidden_size, trans=options.trans)
+                self.inp_trans_q = deploy.nn.OnlineTrans(self.hidden_size, trans=options.trans)
+                self.inp_trans_k = deploy.nn.OnlineTrans(self.hidden_size, trans=options.trans)
+                self.inp_trans_v = deploy.nn.OnlineTrans(self.hidden_size, trans=options.trans)
+        
+        num_heads = self.config.num_attention_heads
+        model_dim = self.config.hidden_size
+        head_dim = model_dim // num_heads
+
+        self.register_buffer("trans_matrix_k", torch.randn([head_dim, head_dim], requires_grad = False))
+        self.register_buffer("trans_matrix_k_inv_t", torch.randn([head_dim, head_dim], requires_grad = False))
+        self.register_buffer("trans_matrix_v", torch.randn([head_dim, head_dim], requires_grad = False))
+        self.register_buffer("kclip_factor_a_max", torch.tensor(4.0))
+        self.register_buffer("kclip_factor_a_min", torch.tensor(4.0))
+        self.register_buffer("vclip_factor_a_max", torch.tensor(4.0))
+        self.register_buffer("vclip_factor_a_min", torch.tensor(4.0))
 
 
+        left_dim, right_dim = get_decompose_dim(self.config.hidden_size)
+        self.register_buffer("left_matrix", torch.randn([left_dim, left_dim], requires_grad = False))
+        self.register_buffer("right_matrix", torch.randn([right_dim, right_dim], requires_grad = False))
+
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        attention_mask: Optional[torch.LongTensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_value: Optional[Cache] = None,
+        output_attentions: bool = False,
+        use_cache: bool = False,
+        cache_position: Optional[torch.LongTensor] = None,
+        **kwargs,
+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
+
+        cache_kwargs = {
+            "sin": kwargs.get("sin"),
+            "cos": kwargs.get("cos"),
+            "cache_position": cache_position,
+            "attention_mask": attention_mask,
+            "trans_matrix_k": self.trans_matrix_k,
+            "trans_matrix_k_inv_t": self.trans_matrix_k_inv_t,
+            "trans_matrix_v": self.trans_matrix_v,
+            "kclip_factor_a_max": self.kclip_factor_a_max,
+            "kclip_factor_a_min": self.kclip_factor_a_min,
+            "vclip_factor_a_max": self.vclip_factor_a_max,
+            "vclip_factor_a_min": self.vclip_factor_a_min,
+            "isFlatQ": self.isFlatQ,
+        }
+
+        return super().forward(
+            hidden_states=hidden_states,
+            attention_mask=attention_mask,
+            position_ids=position_ids,
+            past_key_value=past_key_value,
+            output_attentions=output_attentions,
+            use_cache=use_cache,
+            cache_position=cache_position,
+            cache_kwargs=cache_kwargs,
+        )
+
 class FlatQuantLlamaMLP(LlamaMLP):
     def __init__(self, options, *args, **kwargs):
         super().__init__(*args, **kwargs)
+        self.isFlatQ = hasattr(options, 'trans') and options.trans == "matmul" # if doing FlatQuant
+
         self.options = options
-        self.quantizer = deploy.nn.Quantizer()
+        self.quantizer_g = deploy.nn.Quantizer(lac = self.isFlatQ)
+        self.quantizer_u = deploy.nn.Quantizer(lac = self.isFlatQ)
         self.up_proj = deploy.nn.Linear4bit.from_float(self.up_proj)
         self.gate_proj = deploy.nn.Linear4bit.from_float(self.gate_proj)
         if "down_proj" in self.options.online_trans:
             self.down_proj = torch.nn.Sequential(
                 deploy.nn.OnlineTrans(self.intermediate_size, trans=options.trans),
-                deploy.nn.Quantizer(),
+                deploy.nn.Quantizer(lac = self.isFlatQ),
                 deploy.nn.Linear4bit.from_float(self.down_proj)
             )
         else:
             self.down_proj = torch.nn.Sequential(
-                deploy.nn.Quantizer(),
+                deploy.nn.Quantizer(lac = self.isFlatQ),
                 deploy.nn.Linear4bit.from_float(self.down_proj)
             )
         if "up_gate_proj" in self.options.online_trans:
             if not self.options.fuseLN:
-                self.inp_trans = deploy.nn.OnlineTrans(self.hidden_size, trans=options.trans)
+                self.inp_trans_g = deploy.nn.OnlineTrans(self.hidden_size, trans=options.trans)
+                self.inp_trans_u = deploy.nn.OnlineTrans(self.hidden_size, trans=options.trans)
+        
+        left_dim, right_dim = get_decompose_dim(self.config.hidden_size)
+        self.register_buffer("left_matrix", torch.randn([left_dim, left_dim], requires_grad = False))
+        self.register_buffer("right_matrix", torch.randn([right_dim, right_dim], requires_grad = False))
 
     def forward(self, x):            
-        if not self.options.fuseLN and hasattr(self, "inp_trans"):
-            x = self.inp_trans(x)
-        x = self.quantizer(x)
-        return super().forward(x)
+        if not self.options.fuseLN and hasattr(self, "inp_trans_g"): # isFlatQ
+            x_up = self.up_proj(self.inp_trans_u(x))
+            x_gate = self.gate_proj(self.inp_trans_g(x))
+        else:
+            x = self.quantizer_g(x)
+            x_up = self.up_proj(x)
+            x_gate = self.gate_proj(x)
+        
+        ac = self.act_fn(x_gate)
+        x = x_up * ac
+        x = self.down_proj(x)
+        return x
 
 
 class FlatQuantFP16LlamaForCausalLM(LlamaForCausalLM):
@@ -159,14 +275,18 @@ class FlatQuantFP16LlamaForCausalLM(LlamaForCausalLM):
         if args is not None:
             self.trans = args.trans
             self.online_trans = args.online_trans
+        else:
+            self.trans = "none"
+            self.online_trans = set()
         
     def build_cache(self, batch_size, page_size, max_length):
         device = self.model.layers[0].self_attn.v_proj.weight.device
         dtype = self.cache_dtype or self.model.layers[0].self_attn.v_proj.weight.dtype
         
-        num_heads = self.config.num_key_value_heads
+        num_heads = self.config.num_attention_heads
         model_dim = self.config.hidden_size
-        head_dim = model_dim // num_heads
+        head_dim = model_dim // self.config.num_attention_heads
+        group_size = num_heads // self.config.num_key_value_heads
         disable_quant = self.cache_dtype == "float16" 
         return deploy.transformers.MultiLayerPagedKVCache4Bit(
             batch_size=batch_size,
@@ -179,6 +299,7 @@ class FlatQuantFP16LlamaForCausalLM(LlamaForCausalLM):
             disable_quant=disable_quant,
             trans_dtype=None if disable_quant else torch.float16,
             trans=self.trans if "qk" in self.online_trans else "none",
+            group_size = group_size
         )
 
     def _get_logits_processor(self, generation_config, *args, **kwargs):
@@ -214,3 +335,185 @@ class FlatQuantLlamaForCausalLM(FlatQuantFP16LlamaForCausalLM):
                 layer.post_attention_layernorm = deploy.nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
             layer.mlp = FlatQuantLlamaMLP(options=args, config=config)
         self.cache_dtype = "int4"
+
+        
+    @classmethod
+    def from_pretrained(cls, pretrained_model_name, **kwargs):
+        import os
+        import torch
+        from huggingface_hub import hf_hub_download
+        from safetensors.torch import load_file
+        from safetensors import safe_open
+        import json
+        from transformers.modeling_utils import no_init_weights
+
+        dtype_old = torch.get_default_dtype()
+        torch.set_default_dtype(torch.float16)
+
+        config = cls.config_class.from_pretrained(pretrained_model_name, **kwargs)
+        config._attn_implementation = "flash_attention_2"
+        quant_config = getattr(config, 'quantization_config', {})
+
+        class Args:
+            pass
+
+        args = Args()
+        args.fuseLN = quant_config.get('fuseLN', False)
+        args.trans = quant_config.get('trans', "matmul")
+        args.online_trans = quant_config.get('online_trans', ["qk", "o_proj", "down_proj", "qkv_proj", "up_gate_proj"])
+        args.online_trans = set(args.online_trans)
+
+        with no_init_weights():
+            model = cls(args, config)
+
+        state_dict = {}
+
+        if os.path.isdir(pretrained_model_name):
+            index_path = os.path.join(pretrained_model_name, "model.safetensors.index.json")
+            single_path = os.path.join(pretrained_model_name, "model.safetensors")
+
+            if os.path.exists(index_path):
+                # Sharded model
+                with open(index_path, 'r') as f:
+                    index = json.load(f)
+                
+                loaded_files = set()
+                for tensor_name, filename in index["weight_map"].items():
+                    if filename not in loaded_files:
+                        shard_path = os.path.join(pretrained_model_name, filename)
+                        with safe_open(shard_path, framework = "pt") as f:
+                            for key in f.keys():
+                                if key in index["weight_map"] and index["weight_map"][key] == filename:
+                                    state_dict[key] = f.get_tensor(key)
+                        loaded_files.add(filename)
+                        print(f"Loaded {filename}")
+            else:
+                # Single file
+                state_dict = load_file(single_path)
+        else:
+            # Download from HuggingFace repo
+            try:
+                index_path = hf_hub_download(
+                    repo_id=pretrained_model_name,
+                    filename="model.safetensors.index.json"
+                )
+                
+                # Sharded model - HF
+                with open(index_path, 'r') as f:
+                    index = json.load(f)
+                
+                loaded_files = set()
+                for tensor_name, filename in index["weight_map"].items():
+                    if filename not in loaded_files:
+                        shard_path = hf_hub_download(
+                            repo_id=pretrained_model_name,
+                            filename=filename
+                        )
+                        with safe_open(shard_path, framework = "pt") as f:
+                            for key in f.keys():
+                                if key in index["weight_map"] and index["weight_map"][key] == filename:
+                                    state_dict[key] = f.get_tensor(key)
+                        loaded_files.add(filename)
+                        print(f"Downloaded and loaded {filename}")
+                        
+            except Exception as e:
+                # Single file
+                try:
+                    single_path = hf_hub_download(
+                        repo_id=pretrained_model_name,
+                        filename="model.safetensors",
+                    )
+                    state_dict = load_file(single_path)
+                    print("Downloaded and loaded model.safetensors")
+                except Exception as e2:
+                    raise RuntimeError(f"Failed to load model weights: {e2}")
+    
+        # Reconstruct the checkpoint format from safetensors
+        checkpoint = {
+            "model_state_dict": {},
+            "quantizers": {}
+        }
+        
+        for k, v in state_dict.items():
+            if k.startswith("quantizer."):
+                parts = k.split(".")
+                layer_name = ".".join(parts[1:-1])
+                param_type = parts[-1]
+                
+                if param_type == "scale":
+                    if layer_name not in checkpoint["quantizers"]:
+                        class Quantize:
+                            pass
+                        checkpoint["quantizers"][layer_name] = Quantize()
+                    checkpoint["quantizers"][layer_name].scale = v
+            else:
+                checkpoint["model_state_dict"][k] = v
+
+        new_checkpoint = {
+            "model_state_dict": {},
+            "quantizers": {}
+        }
+        for k, v in checkpoint["model_state_dict"].items():
+            new_k = k.replace("q_proj.linear", "q_proj") \
+                    .replace("q_proj.act_quantizer", "inp_trans_q") \
+                    .replace("k_proj.linear", "k_proj") \
+                    .replace("k_proj.act_quantizer", "inp_trans_k") \
+                    .replace("v_proj.linear", "v_proj") \
+                    .replace("v_proj.act_quantizer", "inp_trans_v") \
+                    .replace("o_proj.linear", "o_proj.1") \
+                    .replace("o_proj.act_quantizer", "o_proj_trans") \
+                    .replace("ln_trans.matrix_left", "left_matrix") \
+                    .replace("ln_trans.matrix_right", "right_matrix") \
+                    .replace("ln_trans", "inp_trans_k") \
+                    .replace("o_trans.matrix", "o_proj_trans.right_matrix") \
+                    .replace("gate_proj.linear", "gate_proj") \
+                    .replace("gate_proj.act_quantizer", "inp_trans_g") \
+                    .replace("up_proj.linear", "up_proj") \
+                    .replace("up_proj.act_quantizer", "inp_trans_u") \
+                    .replace("down_proj.linear", "down_proj.2") \
+                    .replace("down_proj.act_quantizer", "down_proj.0") \
+                    .replace("down_trans.matrix_left", "down_proj.0.left_matrix") \
+                    .replace("down_trans.matrix_right", "down_proj.0.right_matrix")\
+                    .replace("down_trans", "down_proj.0") \
+                    .replace("up_gate_trans.matrix_left", "left_matrix") \
+                    .replace("up_gate_trans.matrix_right", "right_matrix") \
+                    .replace("up_gate_trans", "inp_trans_g") \
+                    .replace("k_cache_quantizer.clip", "kclip") \
+                    .replace("v_cache_quantizer.clip", "vclip") \
+                    .replace("kcache_trans.matrix", "trans_matrix_k") \
+                    .replace("vcache_trans.matrix", "trans_matrix_v")
+            new_checkpoint["model_state_dict"][new_k] = v
+        
+        for k, v in checkpoint["quantizers"].items():
+            new_k = k.replace("linear", "weight_scales") \
+                    .replace("mlp.down_proj.weight_scales", "mlp.down_proj.2.weight_scales") \
+                    .replace("self_attn.o_proj.weight_scales", "self_attn.o_proj.1.weight_scales")
+            new_checkpoint["quantizers"][new_k] = v.scale
+
+        model.load_state_dict(new_checkpoint["model_state_dict"], strict = False)
+        model.load_state_dict(new_checkpoint["quantizers"], strict = False)
+
+        for layer in model.model.layers:    
+            layer.self_attn.inp_trans_q.register_buffer("left_matrix", layer.self_attn.left_matrix)
+            layer.self_attn.inp_trans_k.register_buffer("left_matrix", layer.self_attn.left_matrix)
+            layer.self_attn.inp_trans_v.register_buffer("left_matrix", layer.self_attn.left_matrix)
+            layer.self_attn.inp_trans_q.register_buffer("right_matrix", layer.self_attn.right_matrix)
+            layer.self_attn.inp_trans_k.register_buffer("right_matrix", layer.self_attn.right_matrix)
+            layer.self_attn.inp_trans_v.register_buffer("right_matrix", layer.self_attn.right_matrix)
+
+            layer.mlp.inp_trans_u.register_buffer("left_matrix", layer.mlp.left_matrix)
+            layer.mlp.inp_trans_u.register_buffer("right_matrix", layer.mlp.right_matrix)
+            layer.mlp.inp_trans_g.register_buffer("left_matrix", layer.mlp.left_matrix)
+            layer.mlp.inp_trans_g.register_buffer("right_matrix", layer.mlp.right_matrix)
+
+        for name, module in model.named_modules():
+            for attr_name in ['clip_factor_a_max', 'clip_factor_a_min']:
+                if hasattr(module, attr_name):
+                    attr_value = getattr(module, attr_name)
+                    if isinstance(attr_value, torch.Tensor):
+                        delattr(module, attr_name)
+                        setattr(module, attr_name, attr_value.item())
+
+        torch.set_default_dtype(dtype_old)
+
+        return model
\ No newline at end of file
diff --git a/flatquant/args_utils.py b/flatquant/args_utils.py
index 167b763..0541fcc 100644
--- a/flatquant/args_utils.py
+++ b/flatquant/args_utils.py
@@ -10,13 +10,18 @@ supported_models = [
             './modelzoo/llama-2/llama-2-7b',
             './modelzoo/llama-2/llama-2-13b',
             './modelzoo/llama-2/llama-2-70b',
+            './modelzoo/llama-2-hf/llama-2-7b-hf',
+            './modelzoo/llama-2-hf/llama-2-13b-hf',
+            './modelzoo/llama-2-hf/llama-2-70b-hf',
             './modelzoo/llama-3/llama-3-8b',
             './modelzoo/llama-3/llama-3-70b',
             './modelzoo/llama-3.1/llama-3.1-8b',
             './modelzoo/llama-3.1/llama-3.1-70b',
-            './modelzoo/llama-3.1-instruct/llama-3.1-instruct-8b',
-            './modelzoo/qwen-2.5-instruct/qwen-2.5-instruct-7b',
-            './modelzoo/qwen-2.5-instruct/qwen-2.5-instruct-32b',
+            './modelzoo/llama-3.1-instruct/llama-3.1-8b-instruct',
+            './modelzoo/llama-3-instruct/llama-3-8b-instruct',
+            './modelzoo/llama-3-instruct/llama-3-8b-instruct',
+            './modelzoo/qwen-2.5-instruct/qwen-2.5-7b-instruct',
+            './modelzoo/qwen-2.5-instruct/qwen-2.5-32b-instruct',
             ]
 # supported_models = [
 #             'meta-llama/Llama-2-7b-hf',
@@ -144,6 +149,10 @@ def parser_gen():
         action="store_true",
         help="Distribute the model across multiple GPUs for evaluation.")
 
+    # Add quantized_save flag
+    parser.add_argument('--quantized_save', action = "store_true", default = False,
+                        help = 'Save the quantized model checkpoint.')
+
     args = parser.parse_args()
     if args.a_groupsize > -1:
         raise NotImplementedError
diff --git a/flatquant/data_utils.py b/flatquant/data_utils.py
index 1855f54..72d1e69 100644
--- a/flatquant/data_utils.py
+++ b/flatquant/data_utils.py
@@ -97,9 +97,9 @@ def get_pile(nsamples, seed, seqlen, tokenizer):
 def get_loaders(
     args, name, nsamples=128, seed=0, seqlen=2048, model='', hf_token=None, eval_mode=False
 ):
-    cache_dir = os.path.join(args.cache_dir, name)
-    os.makedirs(cache_dir, exist_ok=True)
-    cached_dataset = os.path.join(cache_dir, "testset.pkl" if eval_mode else f"trainset-{nsamples}-{seed}.pkl")
+    #cache_dir = os.path.join(args.cache_dir, name)
+    #os.makedirs(cache_dir, exist_ok=True)
+    #cached_dataset = os.path.join(cache_dir, "testset.pkl" if eval_mode else f"trainset-{nsamples}-{seed}.pkl")
     # if os.path.exists(cached_dataset):
     if False:
         print(f"Loading cached tokenized dataset at {cached_dataset}...")
diff --git a/flatquant/flat_utils.py b/flatquant/flat_utils.py
index 9c8eed2..1a6e4e2 100644
--- a/flatquant/flat_utils.py
+++ b/flatquant/flat_utils.py
@@ -22,7 +22,7 @@ def reparameterize_ln(ln, trans):
     ln_weight = ln.weight.data
     ori_dtype = ln_weight.dtype
     ln_weight = ln_weight.to(torch.float64)
-    ln_weight = ln_weight * trans.diag_scale.to(torch.float64)
+    ln_weight = ln_weight * trans.diag_scale.to(torch.float64) ## activation에서 on-line으로 하는 대신 ln, upgate weight에 fuse / 논문이랑 달리 diag_scale < 1, 즉 activation엔 곱하고 weight엔 나눔
     ln.weight.data = ln_weight.to(ori_dtype)
     trans.use_diag = False
 
@@ -93,3 +93,114 @@ def load_flat_matrices(args, model, path=None):
     return model
 
 
+
+
+## save weight in uint8 with safetensors
+def save_quantized_weights_with_safetensors(args, model, quantizers, sym = True):
+
+    from deploy.functional import pack_i4
+    import json
+    from safetensors.torch import save_file
+    from huggingface_hub import split_torch_state_dict_into_shards
+
+    state_dict = {}
+    metadata = {}
+    max_shard_size = "5GB"
+    
+    for name, param in model.named_parameters():
+        if name.endswith('.weight') or name.endswith('.bias'):
+            layer_name = name.rsplit('.', 1)[0]
+        else:
+            layer_name = name
+            
+        is_quantized = layer_name in quantizers
+        
+        if is_quantized and 'weight' in name:
+            scale = quantizers[layer_name].scale
+            maxq = quantizers[layer_name].maxq
+            zero = quantizers[layer_name].zero
+            
+            scale = scale.to(param.device)
+            zero = zero.to(param.device)
+            maxq = maxq.to(param.device)
+
+            if sym:
+                param_quant = torch.clamp((param / scale).round(), -(maxq + 1), maxq)
+
+            else:
+                param_quant = torch.clamp((param / scale).round() + zero, 0, maxq)
+            
+            param_quant_int8 = param_quant.to(torch.int8)
+            state_dict[name] = pack_i4(param_quant_int8).contiguous()
+
+        else:
+            state_dict[name] = param.to(torch.half).contiguous()
+    
+    for layer_name, quantizer in quantizers.items():
+        state_dict[f"quantizer.{layer_name}.scale"] = quantizer.scale.contiguous()
+
+        if hasattr(quantizer, 'zero') and quantizer.zero is not None:
+            state_dict[f"quantizer.{layer_name}.zero"] = quantizer.zero.contiguous()
+
+        if hasattr(quantizer, 'maxq') and quantizer.maxq is not None:
+            state_dict[f"quantizer.{layer_name}.maxq"] = quantizer.maxq.contiguous()
+
+    state_dict_split = split_torch_state_dict_into_shards(
+        state_dict, 
+        max_shard_size = max_shard_size,
+        filename_pattern = "model{suffix}.safetensors"
+    )
+
+    save_dir = args.exp_dir
+    os.makedirs(save_dir, exist_ok=True)
+
+    metadata['quantization_config'] = json.dumps({
+        'w_bits': args.w_bits,
+        'model_name': args.model,
+        'symmetric': sym,
+        'format': 'packed_int4'
+    })
+    
+    shards = {}
+    for filename, tensor_names in state_dict_split.filename_to_tensors.items():
+        shard_state_dict = {}
+        for tensor_name in tensor_names:
+            shard_state_dict[tensor_name] = state_dict[tensor_name]
+        shards[filename] = shard_state_dict
+    
+    # Save shards
+    first_shard = True
+    for shard_file, shard_state_dict in shards.items():
+        shard_path = os.path.join(save_dir, shard_file)
+        
+        # Only add metadata to the first file
+        if first_shard:
+            save_file(shard_state_dict, shard_path, metadata=metadata)
+            first_shard = False
+        else:
+            save_file(shard_state_dict, shard_path)
+        print(f"Saved {shard_file}")
+    
+    # Save index
+    if state_dict_split.is_sharded:
+        index = {
+            "metadata": state_dict_split.metadata if hasattr(state_dict_split, 'metadata') else {},
+            "weight_map": state_dict_split.tensor_to_filename
+        }
+        index_path = os.path.join(save_dir, "model.safetensors.index.json")
+        with open(index_path, "w") as f:
+            json.dump(index, f, indent = 2)
+        print(f"Saved index to {index_path}")
+    
+    # Save config
+    config_path = os.path.join(save_dir, "quantization_config.json")
+    with open(config_path, 'w') as f:
+        json.dump({
+            'w_bits': args.w_bits,
+            'model_name': args.model,
+            'symmetric': sym,
+            'format': 'packed_int4',
+            'sharded': state_dict_split.is_sharded
+        }, f, indent=2)
+
+    logging.info("saved weights at {}".format(save_dir))
\ No newline at end of file
diff --git a/flatquant/model_tools/llama_utils.py b/flatquant/model_tools/llama_utils.py
index 6c1f3b4..8afa3f4 100644
--- a/flatquant/model_tools/llama_utils.py
+++ b/flatquant/model_tools/llama_utils.py
@@ -281,7 +281,6 @@ class FlatQuantLlamaAttention(LlamaAttention):
                     attn_output = self.o_proj(attn_output, qa_trans=[attn_o_og_it, attn_v_og_it])
                 else:
                     attn_output = self.o_proj(attn_output)
-
         if not output_attentions:
             attn_weights = None
         return attn_output, attn_weights, past_key_value
diff --git a/get_snapshot_dir.py b/get_snapshot_dir.py
index 452614a..bd92f84 100644
--- a/get_snapshot_dir.py
+++ b/get_snapshot_dir.py
@@ -3,7 +3,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer
 import subprocess
 
 model_id_to_dir = {
-    "meta-llama/Meta-Llama-3-8B": "./modelzoo/llama-3/llama-3-8b",
+    "meta-llama/Llama-2-7B-hf": "./modelzoo/llama-2/llama-2-7b-hf",
 }
 
 for model_id, local_dir in model_id_to_dir.items():
diff --git a/main.py b/main.py
index e84a9e4..4c918f7 100644
--- a/main.py
+++ b/main.py
@@ -47,6 +47,10 @@ def main():
             quantizers = gptq_utils.rtn_fwrd(model, utils.DEV, args)
         save_dict["w_quantizers"] = quantizers
 
+    ## save quantized weight
+    if args.quantized_save:
+        flat_utils.save_quantized_weights_with_safetensors(args, model, quantizers)
+
     if args.distribute_model:
         utils.distribute_model(model)
     else:
diff --git a/requirements.txt b/requirements.txt
index 4a941b7..07942f9 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -3,4 +3,8 @@ torch==2.7.1
 accelerate==0.32.0
 datasets==2.17.1
 lm-eval==0.4.4
-termcolor
\ No newline at end of file
+termcolor
+
+## for Llama2-tokenizer
+sentencepiece
+protobuf
\ No newline at end of file
